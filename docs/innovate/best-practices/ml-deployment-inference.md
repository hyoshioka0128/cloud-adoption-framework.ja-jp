---
title: 展開時の機械学習推論
description: 運用環境に展開されている AI モデルが、どのように予測を行うかについて説明します。
author: DonnaForlin
ms.author: brblanch
ms.date: 01/20/2021
ms.topic: conceptual
ms.service: cloud-adoption-framework
ms.subservice: innovate
ms.custom: think-tank
ms.openlocfilehash: 21d15322033cbc5a6f5ce325c28ef851c261ace4
ms.sourcegitcommit: 51565dc4d3a1858bd62f708f2e4c082fbd4c6fe4
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 04/20/2021
ms.locfileid: "107747910"
---
# <a name="machine-learning-inference-during-deployment"></a>展開時の機械学習推論

運用環境に AI モデルを展開する際には、どのように予測を行うかを考える必要があります。 AI モデルの主なプロセスは以下の 2 つです。

- **バッチ推論:** 観測のバッチに基づいて予測を行う非同期プロセス。 予測は、エンド ユーザーまたはビジネス アプリケーション用のファイルかデータベースに格納されます。

- **リアルタイム (または対話型) の推論:** モデルを解放して、任意のタイミングで予測を行い、即座の応答をトリガーします。 このパターンは、ストリーミングおよび対話型アプリケーションのデータを分析するために使用できます。

次の質問を考慮してモデルを評価し、2 つのプロセスを比較した上で自分のモデルに適したものを選択してください。

- 予測を生成する頻度はどの程度か。
- 結果はどの程度すぐに必要になるか。
- 予測を個別に、小規模なバッチで、または大規模なバッチで生成するかどうか。
- モデルで予想される待機時間があるかどうか。
- モデルを実行するために必要なコンピューティング能力はどの程度なのか。
- モデルの保守にかかる運用上の影響とコストはどの程度あるのか。

次のデシジョン ツリーを使用して、どの展開モデルがご自身のユース ケースに最も適しているかを判断します。

[![リアルタイムまたはバッチ推論のデシジョン ツリーの図。](./media/inference-decision-tree.png)](./media/inference-decision-tree.png#lightbox)

## <a name="batch-inference"></a>バッチ推論

バッチ推論 (オフライン推論とも呼ばれます) は、期間を指定してモデルを実行したり、ビジネス アプリケーションに予測値を保存したりするのに役立つ、よりシンプルな推論プロセスです。

バッチ推論については、次のベスト プラクティスを検討してください。

- **バッチ スコアリングのトリガー:** Azure Machine Learning パイプラインと Azure Machine Learning の `ParallelRunStep` 機能を使用して、スケジュールまたはイベントベースの自動化を設定します。 AI Show に移動して [Azure Machine Learning を使用したバッチ推論を実行`ParallelRunStep`](https://channel9.msdn.com/Shows/AI-Show/How-to-do-Batch-Inference-using-AML-ParallelRunStep)し、プロセスの詳細を確認してください。

- **バッチ推論のコンピューティング オプション:** バッチ推論プロセスは連続して実行されるわけではないため、さまざまなワークロードを処理できる再利用可能なクラスターを自動的に開始、停止、およびスケーリングすることをお勧めします。 モデルによって必要な環境は異なり、また後続のモデルでコンピューティングを利用できるようにするためには、ソリューションによって特定の環境を展開できるだけでなく、推論が終了したときに削除できるようにする必要があります。 モデルに適したコンピューティング インスタンスを特定するには、次のデシジョン ツリーを参照してください。

  [![コンピューティング デシジョン ツリーの図。](./media/compute-decision-tree.png)](./media/compute-decision-tree.png#lightbox)

- **バッチ推論を実装する:** Azure では、バッチ推論用に複数の機能がサポートされています。 その機能の 1 つが Azure Machine Learning の `ParallelRunStep` です。これを使用すると、Azure に保存されているテラバイトもの構造化データまたは非構造化データから分析情報を得ることができます。 `ParallelRunStep` は、すぐに使用できる並列処理を提供し、Azure Machine Learning パイプライン内で動作します。

- **バッチ推論の課題:** バッチ推論は、運用環境でモデルを使用して展開できる簡単な方法ですが、いくつかの課題もあります。

  - 推論が実行される頻度によっては、アクセスした時点で生成されたデータがすでに無意味となっている可能性があります。

  - コールドスタートの問題の一種で、新しいデータに対する結果が得られない可能性があります。 たとえば、新しいユーザーがアカウントを作成し、小売店のレコメンデーション システムを使って買い物を始めた場合、次のバッチ推論が実行されるまで製品のレコメンデーションは利用できません。 これがご自身のユース ケースにとって障害になる場合は、リアルタイムの推論を検討してください。

  - 多くのリージョンへの展開や、高可用性は、バッチ推論のシナリオでは重要な問題ではありません。 モデルをリージョンごとに展開する必要はなく、データ ストアは、多くの場所で高可用性戦略を使用して展開する必要がある場合があります。 通常、これはアプリケーションの HA 設計と戦略に従います。

## <a name="real-time-inference"></a>リアルタイムの推論

リアルタイムまたは対話型の推論は、モデルの推論をいつでもトリガーでき、即座の応答が求められるアーキテクチャです。 このパターンは、ストリーミング データや対話型アプリケーションのデータなどを分析するために使用できます。 このモードを使用すると、リアルタイムで機械学習モデルを活用し、前述のバッチ推論におけるコールドスタートの問題を解決できます。

リアルタイムの推論がモデルに適している場合は、以下の検討事項とベスト プラクティスを参照してください。

- **リアルタイムの推論の課題:** 待機時間とパフォーマンスの要件により、モデルのリアルタイムの推論のアーキテクチャはより複雑になります。 場合によっては、システムは 100 ミリ秒以下で応答する必要がありますが、その間にデータを取得し、推論を実行し、モデル結果を検証して保存し、必要なビジネス ロジックを実行し、その結果をシステムやアプリケーションに返す必要があります。

- **リアルタイムの推論のコンピューティング オプション:** リアルタイムの推論を実装する最良の方法は、モデルをコンテナー形式で Docker や AKS クラスターに展開し、REST API を使用して Web サービスとして公開することです。 これにより、モデルはそれ独自の隔離された環境で実行され、他の Web サービスと同様に管理できるようになります。 その後、Docker または AKS 機能を、管理、監視、スケーリングなどに使用できます。 このモデルは、オンプレミス、クラウド、またはエッジに展開できます。 前述のコンピューティング デシジョンでは、リアルタイムの推論について概説しています。

- **複数リージョンの展開と高可用性:** リアルタイムの推論シナリオでは、リージョンの展開と高可用性アーキテクチャを考慮する必要があります。これは、待機時間とモデルのパフォーマンスを解決することが重要になるためです。 複数リージョンの展開における待機時間を短縮するには、使用されるポイントにできるだけ近い位置にモデルを配置することをお勧めします。 モデルとサポートするインフラストラクチャは、ビジネスの高可用性と DR の原則と戦略に従う必要があります。

## <a name="many-models-scenario"></a>多数モデルのシナリオ

人口統計、ブランド、SKU などの特徴によって顧客行動が大きく変化する可能性があるスーパーマーケットの売上予測など、1 つのモデルでは、現実世界の問題が持つ複雑な性質を捉えることができない場合があります。 地域により、スマート メーターの予測メンテナンスの開発も大幅に異なる可能性があります。 これらのシナリオ用に、地域のデータや店舗レベルの関係を捉えるための多数のモデルを用意することで、1 つのモデルよりも高い精度が得られる可能性があります。 このアプローチでは、このレベルの粒度を実現するために十分なデータが利用可能であることを前提としています。

多数モデルのシナリオは、おおまかに、データ ソース、データ サイエンス、および多数モデルの 3 つの段階で発生します。

[![多数モデルのシナリオの図。](./media/many-models-scenario.png)](./media/many-models-scenario.png#lightbox)

**データ ソース:** データ ソースの段階では、カーディナリティがあまり高くならないようデータをセグメント化することが重要です。 製品 ID またはバーコードをメイン パーティションに含めると、生成されるセグメントが多すぎて意味のあるモデルを阻害する可能性があるため、これらは含めないようにします。 ブランド、SKU、ロイヤリティなどの特徴が、より適切でしょう。 また、データ分布を歪めるような外れ値を取り除くことで、データを均質化することも重要です。

**データ サイエンス:** データ サイエンスの段階では、各データ パーティションに対して複数の実験が並行して実行されます。 これは通常、実験で得られたモデルを評価して最適なものを決定する反復的なプロセスです。

**多数モデル:** 各セグメントまたはカテゴリに最適なモデルは、モデル レジストリに登録されています。 モデルにわかりやすい名前を割り当てることで、推論の際に見つけやすくなります。 必要に応じてタグ付けを使用して、モデルを特定のカテゴリにグループ化します。

## <a name="batch-inference-for-many-models"></a>多数モデルのバッチ推論

多数モデルのバッチ推論では、通常、予測は定期的にスケジュールされて繰り返し実行され、同時に実行される大量のデータを処理できます。 1 つのモデルのシナリオとは異なり、多数モデルでは同時に推論が行われるため、適切なモデルを選択することが重要です。 次の図は、多数モデルにおけるバッチ推論の参照パターンを示しています。

[![多数モデルにおけるバッチ推論の参照パターンの図。](./media/many-models-batch-inference.png)](./media/many-models-batch-inference.png#lightbox)

このパターンの主な目的は、モデルを観察し、複数のモデルを同時に実行して、大規模なデータ ボリュームを処理できる拡張性の高い推論ソリューションを実現することです。 階層型のモデル推論を実現するために、多数モデルをカテゴリに分割することができます。 各カテゴリは、Azure Data Lake のように、独自の推論ストレージを持つことができます。 このパターンを実装する際は、モデルを水平方向および垂直方向にバランスよくスケーリングする必要があります。これは、コストとパフォーマンスに影響するためです。 実行するモデル インスタンスの数が多すぎると、パフォーマンスは向上しますが、コストに影響します。 ハイ スペックなノードを持つインスタンスが少なすぎると、コスト効率は向上しますが、スケーリングに関する問題が発生する可能性があります。

## <a name="real-time-inference-for-many-models"></a>多数モデルにおけるリアルタイムの推論

リアルタイムの多数モデル推論では、通常は REST エンドポイント経由での、低待機時間とオンデマンドの要求が求められます。 これは、外部アプリケーションまたはサービスとモデル間の対話に、標準のインターフェイスが必要な場合に便利です。これは通常、JSON ペイロードを使用した REST インターフェイス経由で行われます。

[![多数モデルによるリアルタイムの推論の図。](./media/many-models-real-time-inference.png)](./media/many-models-real-time-inference.png#lightbox)

このパターンの主な目的は、探索サービスを使用して、サービスとそのメタデータの一覧を識別することです。 これは Azure 関数として実装することができ、クライアントから、サービスに関連するサービスの詳細情報 (セキュリティで保護された REST URI で呼び出し可能) を取得できます。 JSON ペイロードがサービスに送信され、関連するモデルが呼び出されることで、クライアントに JSON 応答が返されます。

各サービスは、複数の要求を同時に処理できるステートレスなマイクロサービスであり、物理的な仮想マシンのリソースに制限されています。 複数のグループが選択されている場合、このサービスで複数のモデルを展開できます。この場合、カテゴリや SKU などの同種のグループ化を使用することが推奨されます。 特定のサービスに対して選択されたサービス要求とモデル間のマッピングは、一般的にはスコア スクリプトを使用して、推論ロジックに組み込む必要があります。 モデルのサイズが比較的小さい場合 (数メガバイト)、パフォーマンス上の理由からメモリに読み込むことをお勧めします。それ以外の場合は、各モデルを要求ごとに動的に読み込むことができます。

## <a name="next-steps"></a>次のステップ

Azure Machine Learning を使用した推論の詳細については、次のリソースを参照してください。

- [バッチ スコアリング用の Azure Machine Learning パイプラインを構築する](/azure/machine-learning/tutorial-pipeline-batch-scoring-classification)
- [Azure Machine Learning デザイナーを使用してバッチ予測を実行する](/azure/machine-learning/how-to-run-batch-predictions-designer)
- [Azure Machine Learning でのバッチ推論](https://techcommunity.microsoft.com/t5/azure-ai/batch-inference-in-azure-machine-learning/ba-p/1417010)
